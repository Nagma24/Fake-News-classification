# -*- coding: utf-8 -*-
"""Fake News classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U2HUTfFufz_lhKin6RERft0QnYO8g-cL
"""

import pandas as pd
from sklearn import model_selection
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/MyDrive/FakeNewsDataset/train - Copy.csv')

df.head()

df.shape

df.info()

"""Drop Nan Values"""

df=df.dropna()

"""Get the Independent Features"""

X=df.drop('label',axis=1)

"""Get the Dependent features"""

y=df['label']

X.shape #No. of rows & no. of columns

X

y.shape #Labels

y

"""# Data Pre-processing"""

messages=X.copy()

messages['text'][1]

messages.reset_index(inplace=True)

messages

import nltk #Natural Language tool kit
import re #Regular Expressions
from nltk.corpus import stopwords

nltk.download('stopwords')

"""For Training Data"""

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
corpus = []
for i in range(0, len(messages)):
    review = re.sub('[^a-zA-Z]', ' ', messages['text'][i])
    review = review.lower()
    review = review.split()
    
    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)

corpus[1]

corpus[1]

import numpy as np
X_final=np.array(corpus)
y_final=np.array(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)

len(corpus),y.shape

X_final.shape,y_final.shape

X_train

import string
# Building a vocabulary of words from the given documents
vocab = {}
for i in range(len(corpus)):
    word_list = []
    for word_new in corpus[i].split():
        if (len(word_new)>2):  
            if word_new in vocab:
                vocab[word_new]+=1
            else:
                vocab[word_new]=1

vocab

import matplotlib.pyplot as plt

num_words = [0 for i in range(max(vocab.values())+1)] 
freq = [i for i in range(max(vocab.values())+1)] 
for key in vocab:
    num_words[vocab[key]]+=1
plt.plot(freq,num_words)
plt.axis([1, 10, 0, 20000])
plt.xlabel("Frequency")
plt.ylabel("No of words")
plt.grid()
plt.show()

from wordcloud import WordCloud

comment_words = " ".join(corpus)+" "

wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                min_font_size = 10).generate(comment_words)
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

sortedVocab = dict(sorted(vocab.items(), key=lambda item: item[1], reverse=True)[:5])

plt.bar(sortedVocab.keys(), sortedVocab.values())
# plt.axis([1, 10, 0, 20000])
plt.xlabel("Most frequent words")
plt.ylabel("Frequency")
plt.grid()
plt.show()

sortedVocab = dict(sorted(vocab.items(), key=lambda item: item[1], reverse=True)[-5:])

plt.bar(sortedVocab.keys(), sortedVocab.values())
plt.xlabel("Least frequent words")
plt.ylabel("Frequency")
plt.grid()
plt.show()

cutoff_freq = 40
# For deciding cutoff frequency
num_words_above_cutoff = len(vocab)-sum(num_words[0:cutoff_freq]) 
print("Number of words with frequency higher than cutoff frequency({}) :".format(cutoff_freq),num_words_above_cutoff)

# Words with frequency higher than cutoff frequency are chosen as features
# (i.e we remove words with low frequencies as they would not be significant )
features = []
for key in vocab:
    if vocab[key] >=cutoff_freq:
        features.append(key)

# To represent training data as word vector counts
X_train_dataset = np.zeros((len(X_train),len(features)))
# This can take some time to complete
for i in range(len(X_train)):
    # print(i) 
    word_list = [ word.strip(string.punctuation).lower() for word in X_train[i].split()]
    for word in word_list:
        if word in features:
            X_train_dataset[i][features.index(word)] += 1

X_train_dataset

# To represent test data as word vector counts
X_test_dataset = np.zeros((len(X_test),len(features)))
for i in range(len(X_test)):
    word_list = [ word.strip(string.punctuation).lower() for word in X_test[i].split()]
    for word in word_list:
        if word in features:
            X_test_dataset[i][features.index(word)] += 1

# Using sklearn's Multinomial Naive Bayes
clf = MultinomialNB()
clf.fit(X_train_dataset,y_train)
Y_test_pred = clf.predict(X_test_dataset)
sklearn_score_train = clf.score(X_train_dataset,y_train)
print("Sklearn's score on training data :",sklearn_score_train)
sklearn_score_test = clf.score(X_test_dataset,y_test)
print("Sklearn's score on testing data :",sklearn_score_test)
print("Classification report for testing data :-")
print(classification_report(y_test, Y_test_pred))